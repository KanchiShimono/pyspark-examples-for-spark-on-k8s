{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # PySpark preprocessing for Wikipedia Japanese airticles"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","import multiprocessing\n","import re\n","import unicodedata\n","from functools import reduce\n","from typing import Any, Callable, List, Optional\n","\n","import MeCab\n","import pyspark.sql.functions as F\n","import pyspark.sql.types as T\n","from pyspark.ml.feature import StopWordsRemover\n","from pyspark.sql import DataFrame as SDF\n","from pyspark.sql import SparkSession\n","from pyspark.sql.window import Window\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NUM_CPU = multiprocessing.cpu_count() - 1\n","WIKI_DATA_URL = 'https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2'\n","WIKI_JSON_PATH = 's3a://bucket/path/to/json/'\n","WIKI_PARQUET_PATH = 's3a://bucket/path/to/parquet/'\n","MECAB_DICT = '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd'\n","OUTPUT_PATH = 's3a://bucket/path/to/final/output/'\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## 関数定義"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compose(*funcs: Callable[..., Any]) -> Callable[..., Any]:\n","    for i, func in enumerate(funcs):\n","        if not callable(func):\n","            raise TypeError(\n","                'Position {} argument {} is not callable'.format(i, func)\n","            )\n","\n","    return reduce(\n","        lambda next_func, prev_func: lambda *args, **kwargs: next_func(\n","            prev_func(*args, **kwargs)\n","        ),\n","        funcs,\n","    )\n","\n","\n","def tokenize(\n","    dict_path: Optional[str] = None,\n","    filter_fn: Optional[Callable[[str], Optional[str]]] = None,\n",") -> Callable[[str], List[str]]:\n","    \"\"\"Tokenize Japanese strings by Mecab\n","\n","    Args:\n","        dict_path (Optional[str], optional): Path to mecab dictionary.\n","            neologd is the one of most famous dictionary for Mecab.\n","            Defaults to None.\n","        filter_fn (Optional[Callable[[str], Optional[str]]], optional):\n","            Filter function applied to string parsed by Mecab.\n","            Defaults to None.\n","\n","    Returns:\n","        Callable[[str], List[str]]: Closure function for parsing Mecab. This is\n","            useful to specify Mecab dictionary path for PySpark UDF.\n","    \"\"\"\n","\n","    if dict_path is None:\n","        mecab_arg = '-Ochasen'\n","    else:\n","        mecab_arg = '-Ochasen -r /dev/null -d {}'.format(dict_path)\n","\n","    def _tokenize(sentence: str) -> List[str]:\n","        tagger = MeCab.Tagger(mecab_arg)\n","        parsed = tagger.parse(sentence).split('\\n')\n","\n","        tokens = []\n","        for s in parsed:\n","            token = filter_fn(s) if filter_fn else s.split('\\t')[0]\n","            if not token:\n","                continue\n","\n","            tokens.append(token.strip().replace(' ', '_'))\n","\n","        return tokens\n","\n","    return _tokenize\n","\n","\n","def mecab_filter_fn(analysed_str: str) -> Optional[str]:\n","    \"\"\"Filter function applied to string generated by Mecab tagger parse method\n","\n","    Args:\n","        analysed_str (str): String generated by Mecab tagger parse method.\n","            e.g.)\n","                - 'こんにちは\\tコンニチハ\\tこんにちは\\t感動詞\\t\\t'\n","                - 'ぼく\\tボク\\tぼく\\t名詞-代名詞-一般\\t\\t'\n","                - 'EOS'\n","                - ''\n","\n","    Returns:\n","        Optional[str]: A word passed filter like 'ぼく'.\n","    \"\"\"\n","    splited = analysed_str.split('\\t')\n","\n","    if len(splited) < 4:\n","        return None\n","\n","    pos = splited[3].split('-')\n","    if (pos[0] == '名詞' and pos[1] != '数') or (pos[0] == '形容詞'):\n","        return splited[0].strip().replace(' ', '_')\n","\n","    return None\n","\n","\n","def normalize(sentence: str) -> str:\n","    return unicodedata.normalize('NFKC', sentence.rstrip()).lower()\n","\n","\n","def remove_html_tag(sentence: str) -> str:\n","    pattern = r'<.*?>'\n","    return re.sub(pattern, '', sentence)\n","\n","\n","def remove_url(sentence: str) -> str:\n","    pattern = r'https?://[0-9a-zA-Z_/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+'\n","    return re.sub(pattern, '', sentence)\n","\n","\n","def remove_symbol(sentence: str) -> str:\n","    pattern = r'[!-/:-@\\[-`{-~\\]：-＠]+'\n","    return re.sub(pattern, ' ', sentence)\n","\n","\n","def replace_number(sentence: str, repl: str = '000') -> str:\n","    pattern = r'(-?[\\d]*\\.?[\\d]+)'\n","    return re.sub(pattern, repl, sentence)\n","\n","\n","def remove_price(sentence: str) -> str:\n","    pattern = r'([\\d]+円)|(￥|¥[\\d]+)'\n","    return re.sub(pattern, '', sentence)\n","\n","\n","def remove_deliv(sentence: str) -> str:\n","    pattern = r'[\\d]+(週間|ヵ月|カ月|か月|ヶ月|日)'\n","    return re.sub(pattern, '', sentence)\n","\n","\n","def remove_point(sentence: str) -> str:\n","    pattern = r'[\\d]+倍'\n","    return re.sub(pattern, '', sentence)\n","\n","\n","def remove_line_feed(sentence: str) -> str:\n","    pattern = r'\\n|\\r|\\rn'\n","    return re.sub(pattern, '', sentence)\n","\n","\n","@F.udf(returnType=T.StringType())\n","def cleansing_udf(sentence):\n","    return compose(\n","        # replace_number,\n","        remove_price,\n","        remove_deliv,\n","        remove_point,\n","        remove_symbol,\n","        normalize,\n","        remove_line_feed,\n","        remove_url,\n","        remove_html_tag,\n","    )(sentence)\n","\n","\n","def calc_idf(num_docs: int) -> Callable[[int], float]:\n","    \"\"\"Caliculate IDF\"\"\"\n","\n","    def _calc_idf(doc_freq: int) -> float:\n","        return math.log((num_docs + 1.0) / (doc_freq + 1.0))\n","\n","    return _calc_idf\n","\n","\n","def calc_tfidf(\n","    df: SDF,\n","    tokens_col: str = 'tokens',\n","    doc_id_col: str = 'doc_id',\n","    word_col: str = 'word',\n",") -> SDF:\n","    \"\"\"Caliculate TF-IDF\n","\n","    Args:\n","        df (SDF): Spark DataFrame contains following columns.\n","            - tokens: List of string that be tokenized like\n","                ['I', 'am', 'from', 'Japan'].\n","            - doc_id: Document identifier for calculationg IDF.\n","        tokens_col (str, optional): Column name of tokens.\n","            Tokens mean list of word.\n","            Defaults to 'tokens'.\n","        doc_id_col (str, optional): Column name of doc_id.\n","            Defaults to 'doc_id'.\n","        word_col (str, optional): Column name for explode tokens_col.\n","            Defaults to 'word'.\n","\n","    Returns:\n","        SDF: Spark DataFrame contains TF, IDF, TF-IDF, L2 normalized TF-IDF.\n","    \"\"\"\n","\n","    unfolded_word = df.select(\n","        F.col(doc_id_col),\n","        F.col(tokens_col),\n","        F.explode(tokens_col).alias(word_col),\n","    )\n","\n","    num_words_in_doc = unfolded_word.groupBy(doc_id_col).agg(\n","        F.count(tokens_col).alias('num_words_in_doc')\n","    )\n","\n","    term_freq = (\n","        unfolded_word.groupBy(doc_id_col, word_col)\n","        .agg(F.count(tokens_col).alias('word_count'))\n","        .join(num_words_in_doc, [doc_id_col], 'inner')\n","        .withColumn('tf', F.col('word_count') / F.col('num_words_in_doc'))\n","    )\n","\n","    doc_freq = unfolded_word.groupBy(word_col).agg(\n","        F.countDistinct(doc_id_col).alias('df')\n","    )\n","\n","    num_docs = unfolded_word.select(doc_id_col).distinct().count()\n","\n","    calc_idf_udf = F.udf(calc_idf(num_docs), T.DoubleType())\n","\n","    idf = doc_freq.withColumn('idf', calc_idf_udf(F.col('df')))\n","\n","    spec_l2_norm = Window.partitionBy(doc_id_col)\n","    tfidf = (\n","        term_freq.join(idf, [word_col], 'inner')\n","        .withColumn('tfidf', F.col('tf') * F.col('idf'))\n","        .withColumn(\n","            'l2_norm',\n","            F.sqrt(F.sum(F.col('tfidf') * F.col('tfidf')).over(spec_l2_norm)),\n","        )\n","        .withColumn('normalized_tfidf', F.col('tfidf') / F.col('l2_norm'))\n","    )\n","\n","    return tfidf\n","\n","\n","def _stopwords() -> List[str]:\n","    alphabet = [chr(i) for i in range(97, 97 + 26)]\n","    hiragana = [chr(i) for i in range(12353, 12438)]\n","    katakana = [chr(i) for i in range(12449, 12538)]\n","    return alphabet + hiragana + katakana\n","\n","\n","def _get_tfidf_top_n(df: SDF, n: int = 10) -> SDF:\n","    tfidf_df = calc_tfidf(\n","        df, tokens_col='removed', doc_id_col='title', word_col='word',\n","    )\n","\n","    spec = Window.partitionBy('title').orderBy(F.desc('normalized_tfidf'))\n","\n","    return (\n","        tfidf_df.withColumn('rank', F.dense_rank().over(spec))\n","        .filter(F.col('rank') <= n)\n","        .groupby('title')\n","        .agg(F.collect_list('word').alias('tfidf_ranked_text'))\n","    )\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Wikipediaデータ準備"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! git clone https://github.com/attardi/wikiextractor\n","! wget $WIKI_DATA_URL\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python ./wikiextractor/WikiExtractor.py -b 32M --processes $NUM_CPU -o data/json --json jawiki-latest-pages-articles.xml.bz2\n","! aws s3 cp ./data/json/ $WIKI_JSON_PATH --recursive\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spark = (\n","    SparkSession\n","    .builder\n","    .master('k8s://https://kubernetes.default.svc.cluster.local:443')\n","    .appName('spark_on_k8s')\n","    .config('spark.kubernetes.container.image', 'kanchishimono/pyspark-worker:latest')\n","    .config('spark.kubernetes.pyspark.pythonVersion', 3)\n","    .config('spark.executor.instances', 2)\n","    .config('spark.kubernetes.namespace', 'notebook')\n","    .config('spark.port.maxRetries', 3)\n","    .config('spark.history.ui.port', True)\n","    .config('spark.ui.enabled', True)\n","    .config('spark.ui.port', 4040)\n","    .config('spark.driver.host', 'bxk70o2.notebook.svc.cluster.local')\n","    .config('spark.driver.port', 29413)\n","    .config('spark.driver.memory', '3G')\n","    .config('spark.driver.cores', 1)\n","    .config('spark.executor.memory', '4G')\n","    .config('spark.executor.cores', 1)\n","    .config('spark.default.parallelism', 10)\n","    .config('spark.sql.shuffle.partitions', 10)\n","    .config('spark.eventLog.compress', True)\n","    .config('spark.eventLog.enabled', True)\n","    .config('spark.eventLog.dir', 'file:///tmp/spark-events')\n","    .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:2.7.3')\n","    .config('spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a', 'org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory')\n","    .getOrCreate()\n",")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["schema = T.StructType(\n","    T.StructField('id', T.LongType()),\n","    T.StructField('url', T.StringType()),\n","    T.StructField('title', T.StringType()),\n","    T.StructField('text', T.StringType()),\n",")\n","\n","raw_df = (\n","    spark\n","    .read\n","    .option('mode', 'FAILFAST')\n","    .schema(schema)\n","    .json(WIKI_JSON_PATH)\n","    .select('id', 'url', 'title', 'text')\n","    .repartition(int(spark.conf.get('spark.sql.shuffle.partitions')))\n",")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["raw_df.write.parquet(WIKI_PARQUET_PATH)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## PySparkを使ったWikipediaデータ処理"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Read dataframe of wikipedia articles\n","wikipedia_df = spark.read.parquet(WIKI_PARQUET_PATH)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cleansing text\n","cleansed_df = wikipedia_df.withColumn(\n","    'cleansed', cleansing_udf(F.col('text'))\n",")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tokenize (split sentence to list of word)\n","tokenize_udf = F.udf(\n","    tokenize(MECAB_DICT, mecab_filter_fn), T.ArrayType(T.StringType())\n",")\n","tokenized_df = cleansed_df.withColumn(\n","    'tokenized', tokenize_udf(F.col('cleansed'))\n",").drop('cleansed')\n","tokenized_df.persist()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stopwords = _stopwords()\n","remover = StopWordsRemover(\n","    inputCol='tokenized', outputCol='removed', stopWords=stopwords\n",")\n","removed_stopwords_df = remover.transform(tokenized_df).drop('tokenized')\n","removed_stopwords_df.persist()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate TF-IDF. This will run shuffle operation.\n","tfidf = _get_tfidf_top_n(removed_stopwords_df, n=20)\n","tfidf.write.parquet(OUTPUT_PATH)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# wrap up\n","tokenized_df.unpersist()\n","removed_stopwords_df.unpersist()\n","\n","spark.stop()\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}
